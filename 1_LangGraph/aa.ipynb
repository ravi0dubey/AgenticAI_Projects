{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langgraph.graph import StateGraph, START,  END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Step1: Declare Model\n",
    "model_openapi= ChatOpenAI(model = \"gpt-4o-mini\",temperature=0, api_key=os.getenv(\"OPEN_API_KEY\") )\n",
    "\n",
    "# Step2: Define JokeState\n",
    "class JokeState(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    explanation: str\n",
    "\n",
    "# Step3: Define function to generate joke\n",
    "def generate_joke(state:JokeState):\n",
    "    topic = state[\"topic\"]\n",
    "    prompt = f'generate a joke on the topic:{topic}'\n",
    "    response = model_openapi.invoke(prompt).content\n",
    "    return {'joke': response}\n",
    "\n",
    "# Step4: Define function to generate explanation of the Joke\n",
    "def generate_explanation_on_joke(state: JokeState):\n",
    "    joke = state[\"joke\"]\n",
    "    prompt = f'generate explanation on the joke: {joke}'\n",
    "    response = model_openapi.invoke(prompt).content\n",
    "    return {'explanation' : response}\n",
    "\n",
    "# Step5: Define Joke Graph\n",
    "joke_graph = StateGraph(JokeState)\n",
    "\n",
    "# Step6: Add Node\n",
    "joke_graph.add_node('generate_joke', generate_joke)\n",
    "joke_graph.add_node('generate_explanation_on_joke', generate_explanation_on_joke)\n",
    "\n",
    "# Step7 : Add Edges\n",
    "joke_graph.add_edge(START,'generate_joke')\n",
    "joke_graph.add_edge('generate_joke','generate_explanation_on_joke')\n",
    "joke_graph.add_edge('generate_explanation_on_joke', END)\n",
    "\n",
    "# Step8 : Adding checkpointer\n",
    "joke_checkpointer = InMemorySaver()\n",
    "\n",
    "# Step9 : Compile the graph\n",
    "joke_workflow = joke_graph.compile(checkpointer=joke_checkpointer)\n",
    "\n",
    "# Step10 : Initial configuration\n",
    "joke_config1 = {'configurable': {'thread_id': '1'}}\n",
    "\n",
    "# Step 11: Define initial state for First topic of the joke\n",
    "initial_state: JokeState = {\n",
    "    'topic' : 'pineapple',\n",
    "    'joke' : ' ',\n",
    "    'explanation': ' ',\n",
    "}\n",
    "\n",
    "# Step 12 : Invoke the Graph\n",
    "joke_final_state = joke_workflow.invoke(initial_state, config= joke_config1)\n",
    "print(joke_final_state)\n",
    "\n",
    "# Step 13 : Print Joke_state\n",
    "print(f'joke_state_ : {joke_workflow.get_state(joke_config1)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33204f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'joke_state_history : {list(joke_workflow.get_state_history(joke_config1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc71f35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step14 : Putting checkpoint id of generate_joke\n",
    "joke_config2 = {'configurable': {'thread_id': '1', \"checkpoint_id\":\"1f0f20c3-b888-6240-8001-4c0d0f7fbc7d\"}}\n",
    "\n",
    "# Step 12 : Invoke the Graph\n",
    "joke_final_state = joke_workflow.invoke( {}, config= joke_config2)\n",
    "print(joke_final_state)\n",
    "\n",
    "# Step 13 : Print Joke_state\n",
    "print(f'joke_state_ : {joke_workflow.get_state( joke_config2)}')\n",
    "print(f'joke_state_history : {list(joke_workflow.get_state_history(joke_config2))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
